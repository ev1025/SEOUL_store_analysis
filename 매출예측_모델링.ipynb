{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Input\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>기준_년분기_코드</th>\n",
       "      <th>점포_수</th>\n",
       "      <th>유사_업종_점포_수</th>\n",
       "      <th>개업_율</th>\n",
       "      <th>개업_점포_수</th>\n",
       "      <th>폐업_률</th>\n",
       "      <th>폐업_점포_수</th>\n",
       "      <th>프랜차이즈_점포_수</th>\n",
       "      <th>상권_라벨</th>\n",
       "      <th>서비스_라벨</th>\n",
       "      <th>...</th>\n",
       "      <th>연령대_20_매출_건수</th>\n",
       "      <th>연령대_30_매출_건수</th>\n",
       "      <th>연령대_40_매출_건수</th>\n",
       "      <th>연령대_50_매출_건수</th>\n",
       "      <th>연령대_60_이상_매출_건수</th>\n",
       "      <th>변화_지표</th>\n",
       "      <th>행정동_라벨</th>\n",
       "      <th>동별_임대료</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20234</td>\n",
       "      <td>583</td>\n",
       "      <td>583</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>262</td>\n",
       "      <td>64</td>\n",
       "      <td>...</td>\n",
       "      <td>1354774</td>\n",
       "      <td>2557324</td>\n",
       "      <td>2157729</td>\n",
       "      <td>2607029</td>\n",
       "      <td>4526402</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>112919</td>\n",
       "      <td>37.514004</td>\n",
       "      <td>126.940269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20224</td>\n",
       "      <td>608</td>\n",
       "      <td>608</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>262</td>\n",
       "      <td>64</td>\n",
       "      <td>...</td>\n",
       "      <td>1289771</td>\n",
       "      <td>2413132</td>\n",
       "      <td>2106826</td>\n",
       "      <td>2297953</td>\n",
       "      <td>2956300</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>112919</td>\n",
       "      <td>37.514004</td>\n",
       "      <td>126.940269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20211</td>\n",
       "      <td>612</td>\n",
       "      <td>612</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>262</td>\n",
       "      <td>64</td>\n",
       "      <td>...</td>\n",
       "      <td>1039750</td>\n",
       "      <td>2502903</td>\n",
       "      <td>2582133</td>\n",
       "      <td>2811751</td>\n",
       "      <td>3284720</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>112919</td>\n",
       "      <td>37.514004</td>\n",
       "      <td>126.940269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20214</td>\n",
       "      <td>1873</td>\n",
       "      <td>1875</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>1070</td>\n",
       "      <td>59</td>\n",
       "      <td>...</td>\n",
       "      <td>206615</td>\n",
       "      <td>291151</td>\n",
       "      <td>351842</td>\n",
       "      <td>298119</td>\n",
       "      <td>206427</td>\n",
       "      <td>0</td>\n",
       "      <td>360</td>\n",
       "      <td>117210</td>\n",
       "      <td>37.533273</td>\n",
       "      <td>126.961560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20214</td>\n",
       "      <td>614</td>\n",
       "      <td>614</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>262</td>\n",
       "      <td>64</td>\n",
       "      <td>...</td>\n",
       "      <td>1460974</td>\n",
       "      <td>2421794</td>\n",
       "      <td>2042804</td>\n",
       "      <td>2239717</td>\n",
       "      <td>2440004</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>112919</td>\n",
       "      <td>37.514004</td>\n",
       "      <td>126.940269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229410</th>\n",
       "      <td>20214</td>\n",
       "      <td>67</td>\n",
       "      <td>67</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>595</td>\n",
       "      <td>99</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>119</td>\n",
       "      <td>95279</td>\n",
       "      <td>37.477525</td>\n",
       "      <td>126.982073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229411</th>\n",
       "      <td>20211</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>236</td>\n",
       "      <td>39</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>109576</td>\n",
       "      <td>37.490081</td>\n",
       "      <td>126.886535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229412</th>\n",
       "      <td>20223</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1118</td>\n",
       "      <td>76</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>118</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>133399</td>\n",
       "      <td>37.576507</td>\n",
       "      <td>127.006507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229413</th>\n",
       "      <td>20231</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1118</td>\n",
       "      <td>76</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>296</td>\n",
       "      <td>133399</td>\n",
       "      <td>37.576507</td>\n",
       "      <td>127.006507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229414</th>\n",
       "      <td>20231</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1248</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>314</td>\n",
       "      <td>61443</td>\n",
       "      <td>37.581009</td>\n",
       "      <td>127.047812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>229415 rows × 179 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        기준_년분기_코드  점포_수  유사_업종_점포_수  개업_율  개업_점포_수  폐업_률  폐업_점포_수  프랜차이즈_점포_수  \\\n",
       "0           20234   583         583     0        0     1        4           0   \n",
       "1           20224   608         608     0        1     0        1           0   \n",
       "2           20211   612         612     1        5     1        4           0   \n",
       "3           20214  1873        1875     1       18     1       22           2   \n",
       "4           20214   614         614     0        1     1        4           0   \n",
       "...           ...   ...         ...   ...      ...   ...      ...         ...   \n",
       "229410      20214    67          67    12        8     2        1           0   \n",
       "229411      20211     5           5    20        1     0        0           0   \n",
       "229412      20223     4           4     0        0     0        0           0   \n",
       "229413      20231     4           4     0        0     0        0           0   \n",
       "229414      20231     5           5    20        1     0        0           0   \n",
       "\n",
       "        상권_라벨  서비스_라벨  ...  연령대_20_매출_건수  연령대_30_매출_건수  연령대_40_매출_건수  \\\n",
       "0         262      64  ...       1354774       2557324       2157729   \n",
       "1         262      64  ...       1289771       2413132       2106826   \n",
       "2         262      64  ...       1039750       2502903       2582133   \n",
       "3        1070      59  ...        206615        291151        351842   \n",
       "4         262      64  ...       1460974       2421794       2042804   \n",
       "...       ...     ...  ...           ...           ...           ...   \n",
       "229410    595      99  ...             0             0             5   \n",
       "229411    236      39  ...             0            20             0   \n",
       "229412   1118      76  ...             0             0           118   \n",
       "229413   1118      76  ...             0             0            47   \n",
       "229414   1248      14  ...             0             0             0   \n",
       "\n",
       "        연령대_50_매출_건수  연령대_60_이상_매출_건수  변화_지표  행정동_라벨  동별_임대료   latitude  \\\n",
       "0            2607029          4526402      0      49  112919  37.514004   \n",
       "1            2297953          2956300      0      49  112919  37.514004   \n",
       "2            2811751          3284720      0      49  112919  37.514004   \n",
       "3             298119           206427      0     360  117210  37.533273   \n",
       "4            2239717          2440004      0      49  112919  37.514004   \n",
       "...              ...              ...    ...     ...     ...        ...   \n",
       "229410             0                0      2     119   95279  37.477525   \n",
       "229411             0                0      0      28  109576  37.490081   \n",
       "229412             0                0      1     296  133399  37.576507   \n",
       "229413             0                0      0     296  133399  37.576507   \n",
       "229414             0                4      0     314   61443  37.581009   \n",
       "\n",
       "         longitude  \n",
       "0       126.940269  \n",
       "1       126.940269  \n",
       "2       126.940269  \n",
       "3       126.961560  \n",
       "4       126.940269  \n",
       "...            ...  \n",
       "229410  126.982073  \n",
       "229411  126.886535  \n",
       "229412  127.006507  \n",
       "229413  127.006507  \n",
       "229414  127.047812  \n",
       "\n",
       "[229415 rows x 179 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/merge_xy.csv',encoding='utf-8-sig')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "당월_매출_금액 평균: 865158748.3650396\n"
     ]
    }
   ],
   "source": [
    "# 평균 계산\n",
    "mean_sales = df['당월_매출_금액'].mean()\n",
    "\n",
    "# 결과 출력\n",
    "print(\"당월_매출_금액 평균:\", mean_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **전처리**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이상치 제거 후 평균 당월 매출 금액: 592568647.680186\n",
      "            기준_년분기_코드  점포_수  유사_업종_점포_수  개업_율  개업_점포_수  폐업_률  폐업_점포_수  \\\n",
      "날짜                                                                      \n",
      "2023-10-01      20234   583         583     0        0     1        4   \n",
      "2022-10-01      20224   608         608     0        1     0        1   \n",
      "2021-01-01      20211   612         612     1        5     1        4   \n",
      "2021-10-01      20214  1873        1875     1       18     1       22   \n",
      "2021-10-01      20214   614         614     0        1     1        4   \n",
      "\n",
      "            프랜차이즈_점포_수  상권_라벨  서비스_라벨  ...  연령대_30_매출_건수  연령대_40_매출_건수  \\\n",
      "날짜                                     ...                               \n",
      "2023-10-01           0    262      64  ...       2557324       2157729   \n",
      "2022-10-01           0    262      64  ...       2413132       2106826   \n",
      "2021-01-01           0    262      64  ...       2502903       2582133   \n",
      "2021-10-01           2   1070      59  ...        291151        351842   \n",
      "2021-10-01           0    262      64  ...       2421794       2042804   \n",
      "\n",
      "            연령대_50_매출_건수  연령대_60_이상_매출_건수  변화_지표  행정동_라벨  동별_임대료   latitude  \\\n",
      "날짜                                                                            \n",
      "2023-10-01       2607029          4526402      0      49  112919  37.514004   \n",
      "2022-10-01       2297953          2956300      0      49  112919  37.514004   \n",
      "2021-01-01       2811751          3284720      0      49  112919  37.514004   \n",
      "2021-10-01        298119           206427      0     360  117210  37.533273   \n",
      "2021-10-01       2239717          2440004      0      49  112919  37.514004   \n",
      "\n",
      "             longitude  로그_당월_매출_금액  \n",
      "날짜                                   \n",
      "2023-10-01  126.940269    27.747894  \n",
      "2022-10-01  126.940269    27.626689  \n",
      "2021-01-01  126.940269    27.594627  \n",
      "2021-10-01  126.961560    27.495556  \n",
      "2021-10-01  126.940269    27.530638  \n",
      "\n",
      "[5 rows x 180 columns]\n"
     ]
    }
   ],
   "source": [
    "# 기준 년분기 코드를 datetime으로 변환하는 함수\n",
    "def convert_quarter_to_date(q_code):\n",
    "    year = str(q_code)[:4]  # 년도 추출\n",
    "    quarter = int(str(q_code)[4])  # 분기 추출\n",
    "    month = (quarter - 1) * 3 + 1  # 분기를 월로 변환 (1, 4, 7, 10)\n",
    "    return pd.Timestamp(year + '-' + str(month) + '-01')\n",
    "\n",
    "# 데이터프레임 df에서 '기준_년분기_코드' 열을 변환\n",
    "df['날짜'] = df['기준_년분기_코드'].apply(convert_quarter_to_date)\n",
    "\n",
    "# 날짜를 인덱스로 설정\n",
    "df.set_index('날짜', inplace=True)\n",
    "\n",
    "# 상권_라벨이 126인 행 제외\n",
    "df = df[df['상권_라벨'] != 126]\n",
    "\n",
    "# 결측치가 50% 이상인 열 삭제\n",
    "df = df.dropna(thresh=len(df) * 0.5, axis=1)\n",
    "\n",
    "# 99% 백분위값 계산 (상권_라벨 + 서비스_라벨 조합별)\n",
    "percentiles = df.groupby(['상권_라벨', '서비스_라벨'])['당월_매출_금액'].quantile(0.99).reset_index()\n",
    "percentiles.columns = ['상권_라벨', '서비스_라벨', '99퍼센타일값']\n",
    "\n",
    "# 이상치 값 대체 (상권 262의 서비스 64번, 상권 1070의 서비스 59번)\n",
    "for (region, service), p99_value in percentiles.set_index(['상권_라벨', '서비스_라벨']).iterrows():\n",
    "    mask = (df['상권_라벨'] == region) & (df['서비스_라벨'] == service)\n",
    "    df.loc[mask & (df['당월_매출_금액'] > p99_value['99퍼센타일값']), '당월_매출_금액'] = p99_value['99퍼센타일값']\n",
    "\n",
    "# 2차 이상치 처리: 평균 ± 2표준편차 기준 제거\n",
    "grouped_stats = df.groupby('서비스_라벨')['당월_매출_금액'].agg(['mean', 'std']).reset_index()\n",
    "grouped_stats.columns = ['서비스_라벨', '평균_매출', '표준편차']\n",
    "grouped_stats['하한'] = (grouped_stats['평균_매출'] - 2 * grouped_stats['표준편차']).clip(lower=0)\n",
    "grouped_stats['상한'] = grouped_stats['평균_매출'] + 2 * grouped_stats['표준편차']\n",
    "\n",
    "# (262, 64), (1070, 59) 제외하고 이상치 제거\n",
    "exclude_combinations = [(262, 64), (1070, 59)]\n",
    "\n",
    "for _, row in grouped_stats.iterrows():\n",
    "    service_label = row['서비스_라벨']\n",
    "    lower_bound = row['하한']\n",
    "    upper_bound = row['상한']\n",
    "    \n",
    "    # 해당 서비스 라벨에 대해 상권-서비스 조합 확인\n",
    "    affected_rows = df[df['서비스_라벨'] == service_label]\n",
    "\n",
    "    for region in affected_rows['상권_라벨'].unique():\n",
    "        if (region, service_label) not in exclude_combinations:\n",
    "            mask = (df['상권_라벨'] == region) & (df['서비스_라벨'] == service_label)\n",
    "            df = df[~(mask & (\n",
    "                (df['당월_매출_금액'] < lower_bound) |\n",
    "                (df['당월_매출_금액'] > upper_bound)\n",
    "            ))]\n",
    "# 이상치 제거 후 평균 계산\n",
    "filtered_mean = df['당월_매출_금액'].mean()\n",
    "print(\"이상치 제거 후 평균 당월 매출 금액:\", filtered_mean)\n",
    "\n",
    "# 로그 변환\n",
    "df['로그_당월_매출_금액'] = np.log1p(df['당월_매출_금액'])\n",
    "\n",
    "# 결과 출력\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "당월_매출_금액 평균: 955768085.1174092\n"
     ]
    }
   ],
   "source": [
    "# 평균 계산\n",
    "mean_sales = df['당월_매출_금액'].mean()\n",
    "\n",
    "# 결과 출력\n",
    "print(\"당월_매출_금액 평균:\", mean_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "클러스터링 결과:\n",
      "            상권_라벨  서비스_라벨  상권_클러스터  서비스_클러스터\n",
      "날짜                                          \n",
      "2023-10-01    262      64      317        46\n",
      "2022-10-01    262      64      317        46\n",
      "2021-01-01    262      64      317        46\n",
      "2021-10-01   1070      59       36        50\n",
      "2021-10-01    262      64      317        46\n",
      "2022-04-01    262      64      317        46\n",
      "2023-01-01    262      64      317        46\n",
      "2022-01-01    262      64      317        46\n",
      "2021-01-01   1070      59       36        50\n",
      "2021-07-01   1070      59       36        50\n"
     ]
    }
   ],
   "source": [
    "# 상권 라벨과 서비스 라벨을 클러스터링하기 위한 데이터 준비\n",
    "X_clustering = df[['상권_라벨', '서비스_라벨']]\n",
    "\n",
    "# KMeans 클러스터링 수행\n",
    "# KMeans 클러스터링 (상권 라벨 500개)\n",
    "kmeans_location = KMeans(n_clusters=500, random_state=42)\n",
    "df['상권_클러스터'] = kmeans_location.fit_predict(X_clustering[['상권_라벨']])\n",
    "\n",
    "# KMeans 클러스터링 (서비스 라벨 60개)\n",
    "kmeans_service = KMeans(n_clusters=60, random_state=42)\n",
    "df['서비스_클러스터'] = kmeans_service.fit_predict(X_clustering[['서비스_라벨']])\n",
    "\n",
    "\n",
    "# 클러스터링 결과 출력 (상위 10개 행)\n",
    "print(\"클러스터링 결과:\")\n",
    "print(df[['상권_라벨', '서비스_라벨', '상권_클러스터', '서비스_클러스터']].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **데이터 생성**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X의 차원: (216758, 10, 1)\n",
      "clusters의 차원: (216758, 2)\n",
      "X의 형태: (216758, 10, 185)\n",
      "y의 형태: (216758, 1)\n"
     ]
    }
   ],
   "source": [
    "# log 데이터 준비\n",
    "log_data = df[['로그_당월_매출_금액']]\n",
    "n_steps = 10  # 타임 스텝 설정\n",
    "\n",
    "# 데이터 준비 함수\n",
    "def create_dataset(data, time_step=1):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_step):\n",
    "        X.append(data[i:(i + time_step)])\n",
    "        y.append(data[i + time_step])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# 데이터셋 생성\n",
    "X, y = create_dataset(log_data.values, time_step=n_steps)\n",
    "\n",
    "# 클러스터 정보를 추가\n",
    "clusters = df[['상권_클러스터', '서비스_클러스터']].values[n_steps:]  # 예측하는 시점의 클러스터 정보\n",
    "\n",
    "# 차원 확인\n",
    "print(\"X의 차원:\", X.shape)  # X의 차원 확인\n",
    "print(\"clusters의 차원:\", clusters.shape)  # clusters의 차원 확인\n",
    "\n",
    "# 클러스터 정보를 3D로 변환\n",
    "clusters = np.repeat(clusters[:, np.newaxis, :], n_steps, axis=1)  # (샘플 수, 타임 스텝, 2)로 변환\n",
    "\n",
    "# 모든 열을 포함하여 X를 생성\n",
    "all_features = df.values[n_steps:]  # 모든 열을 포함한 데이터\n",
    "\n",
    "# X의 차원을 3D로 변환 (타임 스텝을 맞추기 위해)\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)  # (샘플 수, 타임 스텝, 1)\n",
    "\n",
    "# 클러스터 정보를 추가\n",
    "X = np.concatenate((X, clusters), axis=2)  # 기존 X와 클러스터 정보를 결합\n",
    "\n",
    "# all_features를 3D로 변환 (샘플 수, 타임 스텝, 열 수)\n",
    "all_features_3d = np.repeat(all_features[:, np.newaxis, :], n_steps, axis=1)\n",
    "\n",
    "# 모든 열을 결합\n",
    "X = np.concatenate((X, all_features_3d), axis=2)\n",
    "\n",
    "# 생성된 X와 y의 형태 확인\n",
    "print(\"X의 형태:\", X.shape)  # X의 형태를 출력\n",
    "print(\"y의 형태:\", y.shape)  # y의 형태를 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 스케일링\n",
    "scaler = MinMaxScaler()\n",
    "X_reshaped = X.reshape(-1, X.shape[-1])  # 2D로 변환\n",
    "X_scaled = scaler.fit_transform(X_reshaped).reshape(X.shape)  # 다시 3D로 변환\n",
    "\n",
    "# y 스케일링\n",
    "y_scaled = MinMaxScaler().fit_transform(y.reshape(-1,1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분할\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "# X_temp를 검증 데이터와 테스트 데이터로 다시 분할\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **LSTM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 확실한 코드는 아닙니다,,,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 105ms/step - loss: 0.0110\n",
      "Epoch 2/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 107ms/step - loss: 0.0017\n",
      "Epoch 3/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 107ms/step - loss: 3.5984e-04\n",
      "Epoch 4/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 105ms/step - loss: 1.3745e-04\n",
      "Epoch 5/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 118ms/step - loss: 1.2462e-04\n",
      "Epoch 6/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 110ms/step - loss: 1.1441e-04\n",
      "Epoch 7/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 107ms/step - loss: 1.0173e-04\n",
      "Epoch 8/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 102ms/step - loss: 9.9234e-05\n",
      "Epoch 9/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 103ms/step - loss: 1.0089e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 104ms/step - loss: 9.5660e-05\n",
      "Epoch 11/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 104ms/step - loss: 8.6866e-05\n",
      "Epoch 12/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 104ms/step - loss: 8.7149e-05\n",
      "Epoch 13/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 103ms/step - loss: 8.6332e-05\n",
      "Epoch 14/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 104ms/step - loss: 8.5883e-05\n",
      "Epoch 15/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 111ms/step - loss: 8.2883e-05\n",
      "Epoch 16/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 105ms/step - loss: 8.5350e-05\n",
      "Epoch 17/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 106ms/step - loss: 8.3096e-05\n",
      "Epoch 18/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 105ms/step - loss: 8.1464e-05\n",
      "Epoch 19/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 107ms/step - loss: 8.0508e-05\n",
      "Epoch 20/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 105ms/step - loss: 7.9999e-05\n",
      "Epoch 21/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 107ms/step - loss: 8.0485e-05\n",
      "Epoch 22/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 105ms/step - loss: 8.0613e-05\n",
      "Epoch 23/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 108ms/step - loss: 7.9908e-05\n",
      "Epoch 24/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 110ms/step - loss: 7.7829e-05\n",
      "Epoch 25/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 119ms/step - loss: 7.8557e-05\n",
      "Epoch 26/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 105ms/step - loss: 7.7114e-05\n",
      "Epoch 27/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 105ms/step - loss: 8.0348e-05\n",
      "Epoch 28/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 105ms/step - loss: 7.9025e-05\n",
      "Epoch 29/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 104ms/step - loss: 7.4522e-05\n",
      "Epoch 30/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 107ms/step - loss: 7.5734e-05\n",
      "Epoch 31/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 105ms/step - loss: 7.6280e-05\n",
      "Epoch 32/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 103ms/step - loss: 7.4790e-05\n",
      "Epoch 33/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 105ms/step - loss: 7.6160e-05\n",
      "Epoch 34/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 102ms/step - loss: 7.5892e-05\n",
      "Epoch 35/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 108ms/step - loss: 7.4993e-05\n",
      "Epoch 36/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 107ms/step - loss: 7.4473e-05\n",
      "Epoch 37/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 104ms/step - loss: 7.6384e-05\n",
      "Epoch 38/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 105ms/step - loss: 7.6176e-05\n",
      "Epoch 39/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 103ms/step - loss: 7.3665e-05\n",
      "Epoch 40/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 106ms/step - loss: 7.5807e-05\n",
      "Epoch 41/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 105ms/step - loss: 7.3561e-05\n",
      "Epoch 42/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 105ms/step - loss: 7.6298e-05\n",
      "Epoch 43/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 108ms/step - loss: 7.5493e-05\n",
      "Epoch 44/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 108ms/step - loss: 7.3825e-05\n",
      "Epoch 45/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 106ms/step - loss: 7.3775e-05\n",
      "Epoch 46/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 107ms/step - loss: 7.4274e-05\n",
      "Epoch 47/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 105ms/step - loss: 7.4393e-05\n",
      "Epoch 48/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 107ms/step - loss: 7.2838e-05\n",
      "Epoch 49/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 106ms/step - loss: 7.3633e-05\n",
      "Epoch 50/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 108ms/step - loss: 7.1904e-05\n",
      "Epoch 51/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 107ms/step - loss: 7.3098e-05\n",
      "Epoch 52/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 109ms/step - loss: 7.1866e-05\n",
      "Epoch 53/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 113ms/step - loss: 7.4230e-05\n",
      "Epoch 54/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 112ms/step - loss: 7.2210e-05\n",
      "Epoch 55/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 112ms/step - loss: 7.1613e-05\n",
      "Epoch 56/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 108ms/step - loss: 7.4397e-05\n",
      "Epoch 57/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 107ms/step - loss: 7.1817e-05\n",
      "Epoch 58/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 107ms/step - loss: 7.4097e-05\n",
      "Epoch 59/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 107ms/step - loss: 7.3346e-05\n",
      "Epoch 60/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 108ms/step - loss: 7.2143e-05\n",
      "Epoch 61/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 107ms/step - loss: 7.0124e-05\n",
      "Epoch 62/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 109ms/step - loss: 7.0782e-05\n",
      "Epoch 63/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 117ms/step - loss: 7.2937e-05\n",
      "Epoch 64/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 110ms/step - loss: 7.0495e-05\n",
      "Epoch 65/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 110ms/step - loss: 7.1986e-05\n",
      "Epoch 66/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 111ms/step - loss: 6.9857e-05\n",
      "Epoch 67/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 110ms/step - loss: 7.1576e-05\n",
      "Epoch 68/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 110ms/step - loss: 7.0484e-05\n",
      "Epoch 69/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 111ms/step - loss: 7.0719e-05\n",
      "Epoch 70/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 108ms/step - loss: 6.9838e-05\n",
      "Epoch 71/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 112ms/step - loss: 7.1311e-05\n",
      "Epoch 72/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 109ms/step - loss: 6.8790e-05\n",
      "Epoch 73/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 112ms/step - loss: 6.9779e-05\n",
      "Epoch 74/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 108ms/step - loss: 7.2037e-05\n",
      "Epoch 75/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 112ms/step - loss: 6.9989e-05\n",
      "Epoch 76/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 110ms/step - loss: 7.0370e-05\n",
      "Epoch 77/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 109ms/step - loss: 7.1878e-05\n",
      "Epoch 78/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 105ms/step - loss: 7.1165e-05\n",
      "Epoch 79/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 110ms/step - loss: 6.9276e-05\n",
      "Epoch 80/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 106ms/step - loss: 6.9661e-05\n",
      "Epoch 81/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 107ms/step - loss: 7.0069e-05\n",
      "Epoch 82/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 107ms/step - loss: 7.0112e-05\n",
      "Epoch 83/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 107ms/step - loss: 7.1449e-05\n",
      "Epoch 84/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 108ms/step - loss: 7.0297e-05\n",
      "Epoch 85/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 112ms/step - loss: 6.9908e-05\n",
      "Epoch 86/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 113ms/step - loss: 6.8794e-05\n",
      "Epoch 87/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 109ms/step - loss: 6.9488e-05\n",
      "Epoch 88/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 108ms/step - loss: 6.9222e-05\n",
      "Epoch 89/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 110ms/step - loss: 6.9076e-05\n",
      "Epoch 90/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 109ms/step - loss: 6.7787e-05\n",
      "Epoch 91/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 109ms/step - loss: 6.9445e-05\n",
      "Epoch 92/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 111ms/step - loss: 6.9686e-05\n",
      "Epoch 93/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 110ms/step - loss: 6.8942e-05\n",
      "Epoch 94/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 109ms/step - loss: 6.9205e-05\n",
      "Epoch 95/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 109ms/step - loss: 6.8462e-05\n",
      "Epoch 96/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 110ms/step - loss: 6.9819e-05\n",
      "Epoch 97/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 113ms/step - loss: 6.9598e-05\n",
      "Epoch 98/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 112ms/step - loss: 6.9616e-05\n",
      "Epoch 99/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 111ms/step - loss: 6.8057e-05\n",
      "Epoch 100/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 122ms/step - loss: 6.9809e-05\n",
      "\u001b[1m678/678\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 18ms/step\n",
      "y_pred_scaled contains NaN: False\n",
      "y_pred_scaled contains inf: False\n",
      "y_test contains NaN: False\n",
      "y_test contains inf: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_3424\\2489165613.py:56: RuntimeWarning: overflow encountered in exp\n",
      "  y_pred_original = np.exp(y_pred_original_scaled + 1e-10)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_3424\\2489165613.py:60: RuntimeWarning: overflow encountered in exp\n",
      "  y_test_original = np.exp(y_test_original_scaled + 1e-10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 18942986.650852785\n",
      "MAE: 699361.4089352184\n",
      "R²: 0.9833500038704971\n"
     ]
    }
   ],
   "source": [
    "# LSTM 모델 정의\n",
    "model = Sequential()\n",
    "\n",
    "# Input 레이어 추가\n",
    "model.add(Input(shape=(X_train.shape[1], X_train.shape[2])))\n",
    "\n",
    "# 첫 번째 LSTM 층\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(Dropout(0.4))  # 드롭아웃 추가 (과적합 방지)\n",
    "\n",
    "# 두 번째 LSTM 층\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(Dropout(0.4))  # 드롭아웃 추가\n",
    "\n",
    "# # 세 번째 LSTM 층\n",
    "# model.add(LSTM(200, return_sequences=True))\n",
    "# model.add(Dropout(0.4))  # 드롭아웃 추가\n",
    "\n",
    "# 네  번째 LSTM 층\n",
    "model.add(LSTM(64))\n",
    "model.add(Dropout(0.4))  # 드롭아웃 추가\n",
    "\n",
    "# 출력 층\n",
    "model.add(Dense(1))\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# 모델 훈련\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=128)\n",
    "\n",
    "# 예측 수행\n",
    "y_pred_scaled = model.predict(X_test)\n",
    "\n",
    "# NaN 및 inf 확인\n",
    "print(\"y_pred_scaled contains NaN:\", np.isnan(y_pred_scaled).any())\n",
    "print(\"y_pred_scaled contains inf:\", np.isinf(y_pred_scaled).any())\n",
    "\n",
    "# 값 클리핑\n",
    "y_pred_scaled = np.clip(y_pred_scaled, -1e10, 1e10)\n",
    "\n",
    "# y_test를 2차원으로 reshape\n",
    "y_test_reshaped = y_test.reshape(-1, 1)\n",
    "\n",
    "# y_test의 NaN 및 inf 확인\n",
    "print(\"y_test contains NaN:\", np.isnan(y_test_reshaped).any())\n",
    "print(\"y_test contains inf:\", np.isinf(y_test_reshaped).any())\n",
    "\n",
    "# # 값 클리핑\n",
    "# y_test_reshaped = np.clip(y_test_reshaped, -1e10, 1e10)\n",
    "\n",
    "# 역변환: Min-Max 스케일링 복원\n",
    "y_pred_original_scaled = scaler.inverse_transform(np.tile(y_pred_scaled, (1, X_train.shape[2])))\n",
    "\n",
    "# 예측 값의 로그 변환된 값을 역변환\n",
    "y_pred_original = np.exp(y_pred_original_scaled + 1e-10)\n",
    "\n",
    "# y_test도 역변환\n",
    "y_test_original_scaled = scaler.inverse_transform(np.tile(y_test_reshaped, (1, X_train.shape[2])))\n",
    "y_test_original = np.exp(y_test_original_scaled + 1e-10)\n",
    "\n",
    "# 값 클리핑\n",
    "y_pred_original = np.clip(y_pred_original, -1e10, 1e10)\n",
    "y_test_original = np.clip(y_test_original, -1e10, 1e10)\n",
    "\n",
    "# RMSE 계산\n",
    "rmse = np.sqrt(mean_squared_error(y_test_original, y_pred_original))  \n",
    "print(\"RMSE:\", rmse)\n",
    "\n",
    "# MAE 계산\n",
    "mae = mean_absolute_error(y_test_original, y_pred_original) \n",
    "print(\"MAE:\", mae)\n",
    "\n",
    "# R² 계산\n",
    "r_squared = r2_score(y_test_original, y_pred_original)  \n",
    "print(\"R²:\", r_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **GRU** 코드 없음다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DEEPAR+**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\study\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 120ms/step - loss: 0.0117 - val_loss: 1.2138e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 116ms/step - loss: 0.0017 - val_loss: 3.9195e-05\n",
      "Epoch 3/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 119ms/step - loss: 3.6424e-04 - val_loss: 4.3524e-05\n",
      "Epoch 4/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 112ms/step - loss: 1.4489e-04 - val_loss: 1.6889e-05\n",
      "Epoch 5/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 121ms/step - loss: 1.2218e-04 - val_loss: 6.0011e-05\n",
      "Epoch 6/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 112ms/step - loss: 1.1007e-04 - val_loss: 3.2847e-05\n",
      "Epoch 7/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 111ms/step - loss: 1.1285e-04 - val_loss: 3.1632e-05\n",
      "Epoch 8/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 115ms/step - loss: 9.5728e-05 - val_loss: 1.2605e-05\n",
      "Epoch 9/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 111ms/step - loss: 9.4526e-05 - val_loss: 4.5752e-05\n",
      "Epoch 10/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 111ms/step - loss: 9.3535e-05 - val_loss: 1.4899e-05\n",
      "Epoch 11/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 117ms/step - loss: 8.7319e-05 - val_loss: 9.3589e-06\n",
      "Epoch 12/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 116ms/step - loss: 8.7203e-05 - val_loss: 1.4434e-05\n",
      "Epoch 13/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 112ms/step - loss: 8.4288e-05 - val_loss: 7.5662e-06\n",
      "Epoch 14/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 115ms/step - loss: 8.7106e-05 - val_loss: 4.1240e-06\n",
      "Epoch 15/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 134ms/step - loss: 8.4513e-05 - val_loss: 2.3076e-05\n",
      "Epoch 16/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 132ms/step - loss: 8.2952e-05 - val_loss: 1.2986e-05\n",
      "Epoch 17/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 124ms/step - loss: 8.2118e-05 - val_loss: 2.8051e-05\n",
      "Epoch 18/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 122ms/step - loss: 8.1606e-05 - val_loss: 3.8716e-05\n",
      "Epoch 19/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 122ms/step - loss: 8.0321e-05 - val_loss: 1.1102e-05\n",
      "Epoch 20/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 121ms/step - loss: 8.2813e-05 - val_loss: 2.0165e-05\n",
      "Epoch 21/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 121ms/step - loss: 7.7585e-05 - val_loss: 4.9428e-06\n",
      "Epoch 22/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 130ms/step - loss: 8.1500e-05 - val_loss: 1.0019e-05\n",
      "Epoch 23/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 119ms/step - loss: 7.8978e-05 - val_loss: 1.2780e-05\n",
      "Epoch 24/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 116ms/step - loss: 7.9983e-05 - val_loss: 2.9573e-05\n",
      "Epoch 25/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 122ms/step - loss: 7.9015e-05 - val_loss: 8.3696e-06\n",
      "Epoch 26/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 119ms/step - loss: 7.7272e-05 - val_loss: 5.5267e-06\n",
      "Epoch 27/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 132ms/step - loss: 7.7033e-05 - val_loss: 1.3074e-05\n",
      "Epoch 28/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 129ms/step - loss: 7.6735e-05 - val_loss: 6.9169e-06\n",
      "Epoch 29/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 124ms/step - loss: 7.8424e-05 - val_loss: 1.3323e-05\n",
      "Epoch 30/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 125ms/step - loss: 7.7860e-05 - val_loss: 3.0302e-05\n",
      "Epoch 31/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 120ms/step - loss: 7.8342e-05 - val_loss: 1.6074e-05\n",
      "Epoch 32/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 117ms/step - loss: 7.5709e-05 - val_loss: 4.7464e-06\n",
      "Epoch 33/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 120ms/step - loss: 7.7485e-05 - val_loss: 1.4490e-05\n",
      "Epoch 34/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 122ms/step - loss: 7.7073e-05 - val_loss: 5.8520e-06\n",
      "Epoch 35/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 148ms/step - loss: 7.5851e-05 - val_loss: 3.3768e-06\n",
      "Epoch 36/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 134ms/step - loss: 7.3912e-05 - val_loss: 2.0385e-05\n",
      "Epoch 37/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 135ms/step - loss: 7.6405e-05 - val_loss: 6.1695e-06\n",
      "Epoch 38/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 134ms/step - loss: 7.3956e-05 - val_loss: 7.9807e-06\n",
      "Epoch 39/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 115ms/step - loss: 7.4679e-05 - val_loss: 2.6015e-05\n",
      "Epoch 40/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 119ms/step - loss: 7.6804e-05 - val_loss: 2.5084e-06\n",
      "Epoch 41/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 117ms/step - loss: 7.4331e-05 - val_loss: 4.6203e-06\n",
      "Epoch 42/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 131ms/step - loss: 7.3848e-05 - val_loss: 5.9949e-06\n",
      "Epoch 43/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 124ms/step - loss: 7.3481e-05 - val_loss: 1.9718e-05\n",
      "Epoch 44/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 118ms/step - loss: 7.3272e-05 - val_loss: 2.0800e-05\n",
      "Epoch 45/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 113ms/step - loss: 7.4268e-05 - val_loss: 6.1401e-06\n",
      "Epoch 46/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 118ms/step - loss: 7.2345e-05 - val_loss: 2.8683e-06\n",
      "Epoch 47/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 138ms/step - loss: 7.3302e-05 - val_loss: 1.1873e-05\n",
      "Epoch 48/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 117ms/step - loss: 7.4786e-05 - val_loss: 3.8087e-05\n",
      "Epoch 49/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 113ms/step - loss: 7.5270e-05 - val_loss: 4.1656e-06\n",
      "Epoch 50/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 112ms/step - loss: 7.3418e-05 - val_loss: 1.5146e-05\n",
      "Epoch 51/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 117ms/step - loss: 7.2900e-05 - val_loss: 7.5000e-06\n",
      "Epoch 52/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 118ms/step - loss: 7.3791e-05 - val_loss: 1.1600e-05\n",
      "Epoch 53/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 144ms/step - loss: 7.2027e-05 - val_loss: 1.2224e-05\n",
      "Epoch 54/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 145ms/step - loss: 7.3073e-05 - val_loss: 1.1267e-05\n",
      "Epoch 55/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 137ms/step - loss: 7.2873e-05 - val_loss: 4.5490e-06\n",
      "Epoch 56/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 130ms/step - loss: 7.5316e-05 - val_loss: 5.9036e-06\n",
      "Epoch 57/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 118ms/step - loss: 7.1749e-05 - val_loss: 8.1133e-06\n",
      "Epoch 58/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 127ms/step - loss: 7.0942e-05 - val_loss: 5.5693e-06\n",
      "Epoch 59/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 115ms/step - loss: 7.3563e-05 - val_loss: 2.0997e-05\n",
      "Epoch 60/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 119ms/step - loss: 7.1131e-05 - val_loss: 1.0738e-05\n",
      "Epoch 61/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 117ms/step - loss: 7.2683e-05 - val_loss: 2.4217e-06\n",
      "Epoch 62/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 121ms/step - loss: 7.0110e-05 - val_loss: 4.1536e-06\n",
      "Epoch 63/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 141ms/step - loss: 7.0860e-05 - val_loss: 1.4131e-05\n",
      "Epoch 64/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 125ms/step - loss: 7.3034e-05 - val_loss: 7.2119e-06\n",
      "Epoch 65/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 113ms/step - loss: 7.0805e-05 - val_loss: 1.0299e-05\n",
      "Epoch 66/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 117ms/step - loss: 7.1182e-05 - val_loss: 1.7599e-05\n",
      "Epoch 67/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 106ms/step - loss: 7.0735e-05 - val_loss: 4.6502e-06\n",
      "Epoch 68/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 104ms/step - loss: 7.1115e-05 - val_loss: 1.9083e-05\n",
      "Epoch 69/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 101ms/step - loss: 7.0924e-05 - val_loss: 6.7661e-06\n",
      "Epoch 70/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 101ms/step - loss: 7.0299e-05 - val_loss: 1.9291e-06\n",
      "Epoch 71/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 101ms/step - loss: 6.9458e-05 - val_loss: 1.2214e-05\n",
      "Epoch 72/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 103ms/step - loss: 7.0044e-05 - val_loss: 9.9798e-06\n",
      "Epoch 73/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 103ms/step - loss: 7.0909e-05 - val_loss: 3.8509e-06\n",
      "Epoch 74/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 101ms/step - loss: 7.2548e-05 - val_loss: 3.8154e-06\n",
      "Epoch 75/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 102ms/step - loss: 7.1303e-05 - val_loss: 1.2111e-05\n",
      "Epoch 76/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 103ms/step - loss: 7.0394e-05 - val_loss: 3.3047e-06\n",
      "Epoch 77/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 103ms/step - loss: 6.9845e-05 - val_loss: 2.2038e-05\n",
      "Epoch 78/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 103ms/step - loss: 7.3634e-05 - val_loss: 2.8695e-05\n",
      "Epoch 79/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 104ms/step - loss: 7.2968e-05 - val_loss: 4.0719e-06\n",
      "Epoch 80/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 103ms/step - loss: 6.9771e-05 - val_loss: 1.2000e-05\n",
      "Epoch 81/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 105ms/step - loss: 6.9254e-05 - val_loss: 3.3100e-06\n",
      "Epoch 82/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 111ms/step - loss: 7.0952e-05 - val_loss: 3.2611e-06\n",
      "Epoch 83/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 106ms/step - loss: 7.0703e-05 - val_loss: 6.4247e-06\n",
      "Epoch 84/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 107ms/step - loss: 6.9015e-05 - val_loss: 5.2318e-06\n",
      "Epoch 85/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 107ms/step - loss: 7.1498e-05 - val_loss: 5.4645e-06\n",
      "Epoch 86/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 106ms/step - loss: 7.1230e-05 - val_loss: 5.0977e-06\n",
      "Epoch 87/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 107ms/step - loss: 6.8944e-05 - val_loss: 4.6860e-06\n",
      "Epoch 88/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 108ms/step - loss: 7.1096e-05 - val_loss: 3.8761e-06\n",
      "Epoch 89/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 105ms/step - loss: 6.9935e-05 - val_loss: 8.6153e-06\n",
      "Epoch 90/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 106ms/step - loss: 6.9735e-05 - val_loss: 4.6674e-06\n",
      "Epoch 91/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 107ms/step - loss: 6.9984e-05 - val_loss: 1.7967e-06\n",
      "Epoch 92/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 111ms/step - loss: 6.9532e-05 - val_loss: 1.1757e-05\n",
      "Epoch 93/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 108ms/step - loss: 6.9152e-05 - val_loss: 6.1356e-06\n",
      "Epoch 94/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 109ms/step - loss: 6.9580e-05 - val_loss: 5.4056e-06\n",
      "Epoch 95/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 107ms/step - loss: 6.9864e-05 - val_loss: 1.4172e-05\n",
      "Epoch 96/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 108ms/step - loss: 6.9925e-05 - val_loss: 2.5602e-06\n",
      "Epoch 97/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 107ms/step - loss: 6.8875e-05 - val_loss: 6.6608e-06\n",
      "Epoch 98/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 106ms/step - loss: 6.9390e-05 - val_loss: 3.2463e-06\n",
      "Epoch 99/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 109ms/step - loss: 6.8887e-05 - val_loss: 4.3351e-06\n",
      "Epoch 100/100\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 110ms/step - loss: 6.8396e-05 - val_loss: 4.0565e-06\n",
      "\u001b[1m678/678\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 15ms/step\n",
      "RMSE: 0.11920782126126601\n",
      "MAE: 0.0993819897971936\n",
      "R²: 0.9992953883904392\n"
     ]
    }
   ],
   "source": [
    "# DeepAR+ 스타일 모델 정의\n",
    "def create_deep_ar_model(input_shape):\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # LSTM 층 추가\n",
    "    model.add(layers.LSTM(256, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(layers.Dropout(0.4))\n",
    "\n",
    "    model.add(layers.LSTM(128, return_sequences=True))\n",
    "    model.add(layers.Dropout(0.4))\n",
    "\n",
    "    model.add(layers.LSTM(64))\n",
    "    model.add(layers.Dropout(0.4))\n",
    "\n",
    "    # 예측을 위한 출력층\n",
    "    model.add(layers.Dense(1))  # 예측 값\n",
    "    return model\n",
    "\n",
    "# 모델 생성\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])  # (타임 스텝 수, 특성 수)\n",
    "model = create_deep_ar_model(input_shape)\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# 모델 훈련\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=128, validation_data=(X_val, y_val))\n",
    "\n",
    "# 예측 수행\n",
    "y_pred_scaled = model.predict(X_test)\n",
    "\n",
    "# 역변환: 원래 스케일로 복원\n",
    "y_pred = scaler.inverse_transform(np.concatenate((X_test[:, -1, :-1], y_pred_scaled), axis=1))[:, -1]\n",
    "y_test_original = scaler.inverse_transform(np.concatenate((X_test[:, -1, :-1], y_test.reshape(-1, 1)), axis=1))[:, -1]\n",
    "\n",
    "# RMSE 계산\n",
    "rmse = np.sqrt(mean_squared_error(y_test_original, y_pred))\n",
    "print(\"RMSE:\", rmse)\n",
    "\n",
    "# MAE 계산\n",
    "mae = mean_absolute_error(y_test_original, y_pred)\n",
    "print(\"MAE:\", mae)\n",
    "\n",
    "# R² 계산\n",
    "r_squared = r2_score(y_test_original, y_pred)\n",
    "print(\"R²:\", r_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeepAR+ 모델에서 **오차범위 (예: RMSE, MAE, 등)**이 0.2처럼 낮게 나온다면, 몇 가지 이유가 있을 수 있습니다. \n",
    "   1. 데이터의 품질 및 특성\n",
    "데이터의 일관성: 3개년 4분기씩의 데이터는 시간에 따른 패턴이 명확하게 드러날 수 있기 때문에, 모델이 이런 패턴을 잘 학습할 수 있습니다. 특히 시계열 데이터에서 계절성(예: 분기마다 발생하는 패턴)이 뚜렷하면 모델이 이를 잘 잡아낼 수 있습니다.\n",
    "짧은 시계열 데이터: 3년, 즉 12개의 시점만 가지고 있으면 데이터의 복잡성이 적고, 모델이 과적합(overfitting)되는 것을 방지할 수 있습니다. 이는 오히려 모델이 적합하게 잘 학습할 수 있는 환경을 제공할 수 있습니다.\n",
    "좋은 스케일링 및 전처리: 데이터를 잘 전처리하고 스케일링했다면, 모델이 더 효율적으로 학습하고 예측할 수 있습니다.\n",
    "   2. 모델의 성능\n",
    "DeepAR+의 장점: DeepAR+와 같은 시계열 예측 모델은 시계열의 패턴을 잘 학습할 수 있는 구조입니다. 특히 DeepAR+는 시간적 의존성과 특징들 간의 관계를 잘 모델링할 수 있어 예측의 정확도가 높을 수 있습니다.\n",
    "과적합 방지: 모델이 너무 복잡하지 않거나, 적절한 정규화 및 **드롭아웃(dropout)**을 사용했다면, 훈련 데이터에 과적합되지 않고 일반화가 잘 된 결과를 얻을 수 있습니다.\n",
    "   3. 데이터의 양과 다양성\n",
    "데이터의 다양성 부족: 데이터가 3개년 4분기씩밖에 없다면, 모델이 학습할 수 있는 정보의 양이 제한적입니다. 데이터가 적을수록 모델이 패턴을 과도하게 학습할 수 있기 때문에 일반화가 잘되지 않을 수 있습니다. 하지만 그만큼 예측의 오차가 줄어들 수도 있습니다.\n",
    "데이터의 충분성: 반면, 데이터가 간결하고 충분히 유의미한 패턴을 포함하고 있으면 모델이 쉽게 패턴을 찾을 수 있고, 오차가 낮게 나올 수 있습니다.\n",
    "   4. 평가 지표의 선택\n",
    "RMSE나 MAE 같은 평가 지표가 낮다는 것은 모델이 예측을 잘 수행하고 있다는 신호입니다. 그러나 때때로 단기 예측에서 모델이 단기적 패턴에 맞춰 잘 맞춰지는 경우가 많아, 오차가 적게 나올 수 있습니다. 예를 들어, 계절성 또는 분기별 변화를 모델이 잘 포착했다면, 예측 오차가 매우 적을 수 있습니다.\n",
    "   5. 테스트 세트와 검증 세트\n",
    "테스트 데이터가 매우 잘 설정되었거나, 모델이 과도하게 잘 맞춰졌을 수 있습니다. 예를 들어, 테스트 데이터가 훈련 데이터와 비슷하거나 과도하게 잘 조정된 경우 모델이 높은 정확도를 보일 수 있습니다.   \n",
    "   6. **결론**:\n",
    "오차범위가 낮은 이유는 여러 가지가 있을 수 있습니다. 데이터의 특성이나 모델의 학습 방식, 평가 지표 등 다양한 요인이 복합적으로 작용한 결과일 수 있습니다. 데이터가 잘 나왔다면 모델이 잘 예측할 수 있고, 그만큼 DeepAR+ 모델이 패턴을 잘 학습했기 때문에 오차가 낮을 수 있습니다. 다만, 테스트 데이터와 훈련 데이터 간의 차이를 고려하여 과적합되지 않도록 유의하는 것도 중요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
